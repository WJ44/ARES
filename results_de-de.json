[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_de_de.tsv", "ARES_Prediction": 0.5037781579697748, "ARES_Confidence_Interval": [0.441, 0.567], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.874, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_de_de.tsv", "ARES_Prediction": 0.5157981431566337, "ARES_Confidence_Interval": [0.452, 0.579], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.879, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_de_de.tsv", "ARES_Prediction": 0.5532967032967033, "ARES_Confidence_Interval": [0.489, 0.617], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.884, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_de_de.tsv", "ARES_Prediction": 0.5626947679186485, "ARES_Confidence_Interval": [0.499, 0.627], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.887, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_de_de.tsv", "ARES_Prediction": 0.5746364414029085, "ARES_Confidence_Interval": [0.51, 0.639], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.893, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_de_de.tsv", "ARES_Prediction": 0.5875245229177813, "ARES_Confidence_Interval": [0.523, 0.652], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.866, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_de_de.tsv", "ARES_Prediction": 0.6201298701298701, "ARES_Confidence_Interval": [0.555, 0.685], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.879, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_de_de.tsv", "ARES_Prediction": 0.6300539083557951, "ARES_Confidence_Interval": [0.565, 0.695], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.873, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_de_de.tsv", "ARES_Prediction": 0.6399100899100899, "ARES_Confidence_Interval": [0.574, 0.706], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.867, "Annotated_Examples_used_for_PPI": 112}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_de_de.tsv", "ARES_Prediction": 0.5199600798403193, "ARES_Confidence_Interval": [0.489, 0.551], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.98, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_de_de.tsv", "ARES_Prediction": 0.5366876310272537, "ARES_Confidence_Interval": [0.505, 0.568], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.986, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_de_de.tsv", "ARES_Prediction": 0.5626373626373626, "ARES_Confidence_Interval": [0.53, 0.595], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.988, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_de_de.tsv", "ARES_Prediction": 0.5889781859931114, "ARES_Confidence_Interval": [0.556, 0.622], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.986, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_de_de.tsv", "ARES_Prediction": 0.6083832335329341, "ARES_Confidence_Interval": [0.575, 0.641], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.992, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_de_de.tsv", "ARES_Prediction": 0.6329588014981273, "ARES_Confidence_Interval": [0.6, 0.666], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.993, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_de_de.tsv", "ARES_Prediction": 0.6571428571428571, "ARES_Confidence_Interval": [0.624, 0.691], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.994, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_de_de.tsv", "ARES_Prediction": 0.6859838274932615, "ARES_Confidence_Interval": [0.653, 0.719], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.987, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_de_de.tsv", "ARES_Prediction": 0.7090909090909091, "ARES_Confidence_Interval": [0.676, 0.742], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.989, "Annotated_Examples_used_for_PPI": 112}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_de_de.tsv", "ARES_Prediction": 0.499001996007984, "ARES_Confidence_Interval": [0.406, 0.592], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.499, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_de_de.tsv", "ARES_Prediction": 0.49790356394129975, "ARES_Confidence_Interval": [0.405, 0.591], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.525, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_de_de.tsv", "ARES_Prediction": 0.5, "ARES_Confidence_Interval": [0.407, 0.593], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.551, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_de_de.tsv", "ARES_Prediction": 0.5, "ARES_Confidence_Interval": [0.407, 0.593], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.575, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_de_de.tsv", "ARES_Prediction": 0.5, "ARES_Confidence_Interval": [0.407, 0.593], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.6, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_de_de.tsv", "ARES_Prediction": 0.49875156054931336, "ARES_Confidence_Interval": [0.406, 0.591], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.624, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_de_de.tsv", "ARES_Prediction": 0.5, "ARES_Confidence_Interval": [0.407, 0.593], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.651, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_de_de.tsv", "ARES_Prediction": 0.5, "ARES_Confidence_Interval": [0.407, 0.593], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.675, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_de_de.tsv", "ARES_Prediction": 0.4986013986013986, "ARES_Confidence_Interval": [0.406, 0.591], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.699, "Annotated_Examples_used_for_PPI": 112}]]