[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5.tsv", "ARES_Prediction": 0.5412574850299401, "ARES_Confidence_Interval": [0.505, 0.577], "Number_of_Examples_in_Evaluation_Set": 4008, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.911, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525.tsv", "ARES_Prediction": 0.5625576519916142, "ARES_Confidence_Interval": [0.526, 0.599], "Number_of_Examples_in_Evaluation_Set": 3816, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.912, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55.tsv", "ARES_Prediction": 0.5824542124542125, "ARES_Confidence_Interval": [0.546, 0.619], "Number_of_Examples_in_Evaluation_Set": 3640, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575.tsv", "ARES_Prediction": 0.6009223115193264, "ARES_Confidence_Interval": [0.564, 0.637], "Number_of_Examples_in_Evaluation_Set": 3484, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.921, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6.tsv", "ARES_Prediction": 0.625189620758483, "ARES_Confidence_Interval": [0.589, 0.662], "Number_of_Examples_in_Evaluation_Set": 3340, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.925, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625.tsv", "ARES_Prediction": 0.6322347066167291, "ARES_Confidence_Interval": [0.596, 0.669], "Number_of_Examples_in_Evaluation_Set": 3204, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.915, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65.tsv", "ARES_Prediction": 0.6582034632034631, "ARES_Confidence_Interval": [0.621, 0.695], "Number_of_Examples_in_Evaluation_Set": 3080, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.93, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675.tsv", "ARES_Prediction": 0.6805570530098831, "ARES_Confidence_Interval": [0.644, 0.717], "Number_of_Examples_in_Evaluation_Set": 2968, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.94, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7.tsv", "ARES_Prediction": 0.7005361305361305, "ARES_Confidence_Interval": [0.664, 0.737], "Number_of_Examples_in_Evaluation_Set": 2860, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5.tsv", "ARES_Prediction": 0.5186027944111776, "ARES_Confidence_Interval": [0.483, 0.554], "Number_of_Examples_in_Evaluation_Set": 4008, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.92, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525.tsv", "ARES_Prediction": 0.5333123689727464, "ARES_Confidence_Interval": [0.498, 0.569], "Number_of_Examples_in_Evaluation_Set": 3816, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.92, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55.tsv", "ARES_Prediction": 0.5553479853479854, "ARES_Confidence_Interval": [0.52, 0.591], "Number_of_Examples_in_Evaluation_Set": 3640, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.917, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575.tsv", "ARES_Prediction": 0.583681592039801, "ARES_Confidence_Interval": [0.548, 0.62], "Number_of_Examples_in_Evaluation_Set": 3484, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.929, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6.tsv", "ARES_Prediction": 0.6016367265469063, "ARES_Confidence_Interval": [0.566, 0.638], "Number_of_Examples_in_Evaluation_Set": 3340, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.94, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625.tsv", "ARES_Prediction": 0.6321098626716605, "ARES_Confidence_Interval": [0.596, 0.668], "Number_of_Examples_in_Evaluation_Set": 3204, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.921, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65.tsv", "ARES_Prediction": 0.6483549783549784, "ARES_Confidence_Interval": [0.612, 0.684], "Number_of_Examples_in_Evaluation_Set": 3080, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.929, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675.tsv", "ARES_Prediction": 0.6652380952380953, "ARES_Confidence_Interval": [0.629, 0.701], "Number_of_Examples_in_Evaluation_Set": 2968, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7.tsv", "ARES_Prediction": 0.6726806526806527, "ARES_Confidence_Interval": [0.636, 0.709], "Number_of_Examples_in_Evaluation_Set": 2860, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.925, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5.tsv", "ARES_Prediction": 0.46328343313373255, "ARES_Confidence_Interval": [0.407, 0.52], "Number_of_Examples_in_Evaluation_Set": 4008, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.741, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525.tsv", "ARES_Prediction": 0.4731865828092243, "ARES_Confidence_Interval": [0.416, 0.53], "Number_of_Examples_in_Evaluation_Set": 3816, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.748, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55.tsv", "ARES_Prediction": 0.4841208791208791, "ARES_Confidence_Interval": [0.427, 0.541], "Number_of_Examples_in_Evaluation_Set": 3640, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.766, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575.tsv", "ARES_Prediction": 0.49205510907003447, "ARES_Confidence_Interval": [0.435, 0.549], "Number_of_Examples_in_Evaluation_Set": 3484, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.781, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6.tsv", "ARES_Prediction": 0.5129341317365269, "ARES_Confidence_Interval": [0.456, 0.57], "Number_of_Examples_in_Evaluation_Set": 3340, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.791, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625.tsv", "ARES_Prediction": 0.5215106117353309, "ARES_Confidence_Interval": [0.465, 0.578], "Number_of_Examples_in_Evaluation_Set": 3204, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.791, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65.tsv", "ARES_Prediction": 0.5335714285714286, "ARES_Confidence_Interval": [0.477, 0.59], "Number_of_Examples_in_Evaluation_Set": 3080, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.82, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675.tsv", "ARES_Prediction": 0.5453638814016172, "ARES_Confidence_Interval": [0.489, 0.602], "Number_of_Examples_in_Evaluation_Set": 2968, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.827, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7.tsv", "ARES_Prediction": 0.5579720279720279, "ARES_Confidence_Interval": [0.501, 0.615], "Number_of_Examples_in_Evaluation_Set": 2860, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.839, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5.tsv", "ARES_Prediction": 0.5066766467065869, "ARES_Confidence_Interval": [0.456, 0.558], "Number_of_Examples_in_Evaluation_Set": 4008, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.79, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525.tsv", "ARES_Prediction": 0.5213626834381551, "ARES_Confidence_Interval": [0.47, 0.572], "Number_of_Examples_in_Evaluation_Set": 3816, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.787, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55.tsv", "ARES_Prediction": 0.5586446886446886, "ARES_Confidence_Interval": [0.507, 0.61], "Number_of_Examples_in_Evaluation_Set": 3640, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.802, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575.tsv", "ARES_Prediction": 0.5537160352085725, "ARES_Confidence_Interval": [0.502, 0.605], "Number_of_Examples_in_Evaluation_Set": 3484, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.784, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6.tsv", "ARES_Prediction": 0.5585229540918164, "ARES_Confidence_Interval": [0.507, 0.61], "Number_of_Examples_in_Evaluation_Set": 3340, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.768, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625.tsv", "ARES_Prediction": 0.588601747815231, "ARES_Confidence_Interval": [0.537, 0.64], "Number_of_Examples_in_Evaluation_Set": 3204, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.769, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65.tsv", "ARES_Prediction": 0.586017316017316, "ARES_Confidence_Interval": [0.534, 0.638], "Number_of_Examples_in_Evaluation_Set": 3080, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.752, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675.tsv", "ARES_Prediction": 0.6326235399820306, "ARES_Confidence_Interval": [0.581, 0.684], "Number_of_Examples_in_Evaluation_Set": 2968, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.768, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7.tsv", "ARES_Prediction": 0.6314219114219114, "ARES_Confidence_Interval": [0.579, 0.683], "Number_of_Examples_in_Evaluation_Set": 2860, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.733, "Annotated_Examples_used_for_PPI": 300}]]