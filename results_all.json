[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5.tsv", "ARES_Prediction": 0.49686626746506984, "ARES_Confidence_Interval": [0.461, 0.532], "Number_of_Examples_in_Evaluation_Set": 4008, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.896, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525.tsv", "ARES_Prediction": 0.5146436058700209, "ARES_Confidence_Interval": [0.479, 0.55], "Number_of_Examples_in_Evaluation_Set": 3816, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.904, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55.tsv", "ARES_Prediction": 0.5405311355311355, "ARES_Confidence_Interval": [0.505, 0.576], "Number_of_Examples_in_Evaluation_Set": 3640, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.909, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575.tsv", "ARES_Prediction": 0.5638499808649062, "ARES_Confidence_Interval": [0.528, 0.6], "Number_of_Examples_in_Evaluation_Set": 3484, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.906, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6.tsv", "ARES_Prediction": 0.5848902195608782, "ARES_Confidence_Interval": [0.549, 0.621], "Number_of_Examples_in_Evaluation_Set": 3340, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.914, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625.tsv", "ARES_Prediction": 0.5904244694132335, "ARES_Confidence_Interval": [0.554, 0.627], "Number_of_Examples_in_Evaluation_Set": 3204, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.897, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65.tsv", "ARES_Prediction": 0.6110606060606061, "ARES_Confidence_Interval": [0.575, 0.647], "Number_of_Examples_in_Evaluation_Set": 3080, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.902, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675.tsv", "ARES_Prediction": 0.6457457322551662, "ARES_Confidence_Interval": [0.609, 0.682], "Number_of_Examples_in_Evaluation_Set": 2968, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.916, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7.tsv", "ARES_Prediction": 0.6479487179487179, "ARES_Confidence_Interval": [0.611, 0.684], "Number_of_Examples_in_Evaluation_Set": 2860, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.883, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5.tsv", "ARES_Prediction": 0.49272455089820355, "ARES_Confidence_Interval": [0.469, 0.517], "Number_of_Examples_in_Evaluation_Set": 4008, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.98, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525.tsv", "ARES_Prediction": 0.5138050314465409, "ARES_Confidence_Interval": [0.49, 0.538], "Number_of_Examples_in_Evaluation_Set": 3816, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.982, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55.tsv", "ARES_Prediction": 0.5385164835164835, "ARES_Confidence_Interval": [0.514, 0.563], "Number_of_Examples_in_Evaluation_Set": 3640, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.984, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575.tsv", "ARES_Prediction": 0.5655338691159586, "ARES_Confidence_Interval": [0.541, 0.59], "Number_of_Examples_in_Evaluation_Set": 3484, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.982, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6.tsv", "ARES_Prediction": 0.5865868263473054, "ARES_Confidence_Interval": [0.562, 0.611], "Number_of_Examples_in_Evaluation_Set": 3340, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.984, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625.tsv", "ARES_Prediction": 0.6117103620474407, "ARES_Confidence_Interval": [0.587, 0.637], "Number_of_Examples_in_Evaluation_Set": 3204, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.986, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65.tsv", "ARES_Prediction": 0.6342207792207792, "ARES_Confidence_Interval": [0.609, 0.659], "Number_of_Examples_in_Evaluation_Set": 3080, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.984, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675.tsv", "ARES_Prediction": 0.6538544474393531, "ARES_Confidence_Interval": [0.629, 0.679], "Number_of_Examples_in_Evaluation_Set": 2968, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.981, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7.tsv", "ARES_Prediction": 0.6834965034965035, "ARES_Confidence_Interval": [0.659, 0.708], "Number_of_Examples_in_Evaluation_Set": 2860, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.985, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5.tsv", "ARES_Prediction": 0.49974051896207583, "ARES_Confidence_Interval": [0.428, 0.572], "Number_of_Examples_in_Evaluation_Set": 4008, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.6, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525.tsv", "ARES_Prediction": 0.5073165618448637, "ARES_Confidence_Interval": [0.435, 0.579], "Number_of_Examples_in_Evaluation_Set": 3816, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.608, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55.tsv", "ARES_Prediction": 0.5153296703296704, "ARES_Confidence_Interval": [0.443, 0.587], "Number_of_Examples_in_Evaluation_Set": 3640, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.626, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575.tsv", "ARES_Prediction": 0.5163834672789896, "ARES_Confidence_Interval": [0.444, 0.588], "Number_of_Examples_in_Evaluation_Set": 3484, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.623, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6.tsv", "ARES_Prediction": 0.5280838323353293, "ARES_Confidence_Interval": [0.456, 0.6], "Number_of_Examples_in_Evaluation_Set": 3340, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.634, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625.tsv", "ARES_Prediction": 0.5291760299625468, "ARES_Confidence_Interval": [0.457, 0.601], "Number_of_Examples_in_Evaluation_Set": 3204, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.633, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65.tsv", "ARES_Prediction": 0.5268181818181819, "ARES_Confidence_Interval": [0.455, 0.599], "Number_of_Examples_in_Evaluation_Set": 3080, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.635, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675.tsv", "ARES_Prediction": 0.551266846361186, "ARES_Confidence_Interval": [0.479, 0.624], "Number_of_Examples_in_Evaluation_Set": 2968, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.67, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7.tsv", "ARES_Prediction": 0.53993006993007, "ARES_Confidence_Interval": [0.468, 0.612], "Number_of_Examples_in_Evaluation_Set": 2860, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.652, "Annotated_Examples_used_for_PPI": 300}]]