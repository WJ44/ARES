[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "baselines/cora_xorqa_ja.tsv", "ARES_Prediction": 0.8157431457431457, "ARES_Confidence_Interval": [0.772, 0.859], "Number_of_Examples_in_Evaluation_Set": 693, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "baselines/cora_xorqa_ja.tsv", "ARES_Prediction": 0.954920634920635, "ARES_Confidence_Interval": [0.92, 0.989], "Number_of_Examples_in_Evaluation_Set": 693, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "baselines/cora_xorqa_ja.tsv", "ARES_Prediction": 0.5784848484848485, "ARES_Confidence_Interval": [0.517, 0.64], "Number_of_Examples_in_Evaluation_Set": 693, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "baselines/cora_xorqa_ja.tsv", "ARES_Prediction": 0.8942424242424243, "ARES_Confidence_Interval": [0.836, 0.952], "Number_of_Examples_in_Evaluation_Set": 693, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300}]]