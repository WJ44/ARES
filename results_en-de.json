[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_de.tsv", "ARES_Prediction": 0.4939585115483319, "ARES_Confidence_Interval": [0.433, 0.555], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.903, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_de.tsv", "ARES_Prediction": 0.5131214435459718, "ARES_Confidence_Interval": [0.452, 0.574], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.91, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_de.tsv", "ARES_Prediction": 0.5364010989010989, "ARES_Confidence_Interval": [0.475, 0.598], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.913, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_de.tsv", "ARES_Prediction": 0.5749446449073314, "ARES_Confidence_Interval": [0.513, 0.637], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.917, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_de.tsv", "ARES_Prediction": 0.598150128314799, "ARES_Confidence_Interval": [0.536, 0.66], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.925, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_de.tsv", "ARES_Prediction": 0.6044341894060995, "ARES_Confidence_Interval": [0.542, 0.667], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.905, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_de.tsv", "ARES_Prediction": 0.6258116883116883, "ARES_Confidence_Interval": [0.563, 0.689], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.909, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_de.tsv", "ARES_Prediction": 0.6612196765498652, "ARES_Confidence_Interval": [0.599, 0.724], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_de.tsv", "ARES_Prediction": 0.6522852147852147, "ARES_Confidence_Interval": [0.589, 0.715], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.892, "Annotated_Examples_used_for_PPI": 112}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_de.tsv", "ARES_Prediction": 0.49805745651554034, "ARES_Confidence_Interval": [0.455, 0.541], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.981, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_de.tsv", "ARES_Prediction": 0.5204215333932315, "ARES_Confidence_Interval": [0.477, 0.564], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.975, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_de.tsv", "ARES_Prediction": 0.5449175824175825, "ARES_Confidence_Interval": [0.501, 0.589], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.984, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_de.tsv", "ARES_Prediction": 0.5777534033131049, "ARES_Confidence_Interval": [0.533, 0.622], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.968, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_de.tsv", "ARES_Prediction": 0.593466638152267, "ARES_Confidence_Interval": [0.549, 0.638], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.974, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_de.tsv", "ARES_Prediction": 0.6177880328161227, "ARES_Confidence_Interval": [0.573, 0.663], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.986, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_de.tsv", "ARES_Prediction": 0.640422077922078, "ARES_Confidence_Interval": [0.595, 0.686], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.973, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_de.tsv", "ARES_Prediction": 0.6501010781671159, "ARES_Confidence_Interval": [0.605, 0.696], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.976, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_de.tsv", "ARES_Prediction": 0.6791833166833167, "ARES_Confidence_Interval": [0.634, 0.725], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.985, "Annotated_Examples_used_for_PPI": 112}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_de.tsv", "ARES_Prediction": 0.5341638152266894, "ARES_Confidence_Interval": [0.442, 0.626], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.678, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_de.tsv", "ARES_Prediction": 0.5608902365977837, "ARES_Confidence_Interval": [0.469, 0.653], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.674, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_de.tsv", "ARES_Prediction": 0.5704670329670329, "ARES_Confidence_Interval": [0.478, 0.663], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.681, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_de.tsv", "ARES_Prediction": 0.5587071510578974, "ARES_Confidence_Interval": [0.466, 0.651], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.638, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_de.tsv", "ARES_Prediction": 0.5772775876817793, "ARES_Confidence_Interval": [0.484, 0.67], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.637, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_de.tsv", "ARES_Prediction": 0.5955167647583378, "ARES_Confidence_Interval": [0.502, 0.689], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.618, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_de.tsv", "ARES_Prediction": 0.5686688311688312, "ARES_Confidence_Interval": [0.475, 0.662], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.584, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_de.tsv", "ARES_Prediction": 0.6086590296495957, "ARES_Confidence_Interval": [0.515, 0.703], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.62, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_de.tsv", "ARES_Prediction": 0.5953421578421578, "ARES_Confidence_Interval": [0.501, 0.69], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.583, "Annotated_Examples_used_for_PPI": 112}]]