[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5.tsv", "ARES_Prediction": 0.5024673166408155, "ARES_Confidence_Interval": [0.469, 0.536], "Number_of_Examples_in_Evaluation_Set": 36104, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.907, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525.tsv", "ARES_Prediction": 0.5225302466263378, "ARES_Confidence_Interval": [0.489, 0.556], "Number_of_Examples_in_Evaluation_Set": 34384, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.906, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55.tsv", "ARES_Prediction": 0.5427141159623412, "ARES_Confidence_Interval": [0.509, 0.576], "Number_of_Examples_in_Evaluation_Set": 32821, "Ground_Truth_Performance": 0.55, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.905, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575.tsv", "ARES_Prediction": 0.563001847486781, "ARES_Confidence_Interval": [0.529, 0.597], "Number_of_Examples_in_Evaluation_Set": 31394, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.904, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6.tsv", "ARES_Prediction": 0.5835843914112877, "ARES_Confidence_Interval": [0.55, 0.617], "Number_of_Examples_in_Evaluation_Set": 30086, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.903, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625.tsv", "ARES_Prediction": 0.6042190908146661, "ARES_Confidence_Interval": [0.571, 0.638], "Number_of_Examples_in_Evaluation_Set": 28883, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.902, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65.tsv", "ARES_Prediction": 0.624709779634164, "ARES_Confidence_Interval": [0.591, 0.658], "Number_of_Examples_in_Evaluation_Set": 27772, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.901, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675.tsv", "ARES_Prediction": 0.6450233706016528, "ARES_Confidence_Interval": [0.611, 0.679], "Number_of_Examples_in_Evaluation_Set": 26743, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.9, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7.tsv", "ARES_Prediction": 0.6654940282301846, "ARES_Confidence_Interval": [0.632, 0.699], "Number_of_Examples_in_Evaluation_Set": 25788, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.899, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5.tsv", "ARES_Prediction": 0.5133599231848733, "ARES_Confidence_Interval": [0.491, 0.536], "Number_of_Examples_in_Evaluation_Set": 36104, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.981, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525.tsv", "ARES_Prediction": 0.5372153714906157, "ARES_Confidence_Interval": [0.515, 0.56], "Number_of_Examples_in_Evaluation_Set": 34384, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.982, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55.tsv", "ARES_Prediction": 0.5615734844560901, "ARES_Confidence_Interval": [0.539, 0.584], "Number_of_Examples_in_Evaluation_Set": 32821, "Ground_Truth_Performance": 0.55, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.982, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575.tsv", "ARES_Prediction": 0.5857694676265104, "ARES_Confidence_Interval": [0.563, 0.608], "Number_of_Examples_in_Evaluation_Set": 31394, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.983, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6.tsv", "ARES_Prediction": 0.6100607148396817, "ARES_Confidence_Interval": [0.588, 0.632], "Number_of_Examples_in_Evaluation_Set": 30086, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.983, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625.tsv", "ARES_Prediction": 0.6341196089972186, "ARES_Confidence_Interval": [0.612, 0.656], "Number_of_Examples_in_Evaluation_Set": 28883, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.983, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65.tsv", "ARES_Prediction": 0.6582015459215517, "ARES_Confidence_Interval": [0.636, 0.681], "Number_of_Examples_in_Evaluation_Set": 27772, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.984, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675.tsv", "ARES_Prediction": 0.6823147490308991, "ARES_Confidence_Interval": [0.66, 0.705], "Number_of_Examples_in_Evaluation_Set": 26743, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.984, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7.tsv", "ARES_Prediction": 0.7064898402357686, "ARES_Confidence_Interval": [0.684, 0.729], "Number_of_Examples_in_Evaluation_Set": 25788, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.985, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5.tsv", "ARES_Prediction": 0.5396436221286653, "ARES_Confidence_Interval": [0.471, 0.608], "Number_of_Examples_in_Evaluation_Set": 36104, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.605, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525.tsv", "ARES_Prediction": 0.5447250659221343, "ARES_Confidence_Interval": [0.476, 0.613], "Number_of_Examples_in_Evaluation_Set": 34384, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.611, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55.tsv", "ARES_Prediction": 0.5512235052760935, "ARES_Confidence_Interval": [0.483, 0.62], "Number_of_Examples_in_Evaluation_Set": 32821, "Ground_Truth_Performance": 0.55, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.617, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575.tsv", "ARES_Prediction": 0.5566634813446306, "ARES_Confidence_Interval": [0.488, 0.625], "Number_of_Examples_in_Evaluation_Set": 31394, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.623, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6.tsv", "ARES_Prediction": 0.5624534113319595, "ARES_Confidence_Interval": [0.494, 0.631], "Number_of_Examples_in_Evaluation_Set": 30086, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.629, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625.tsv", "ARES_Prediction": 0.5676437119874436, "ARES_Confidence_Interval": [0.499, 0.636], "Number_of_Examples_in_Evaluation_Set": 28883, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.636, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65.tsv", "ARES_Prediction": 0.5716169763310768, "ARES_Confidence_Interval": [0.503, 0.64], "Number_of_Examples_in_Evaluation_Set": 27772, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.644, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675.tsv", "ARES_Prediction": 0.5779825250221241, "ARES_Confidence_Interval": [0.509, 0.647], "Number_of_Examples_in_Evaluation_Set": 26743, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.649, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7.tsv", "ARES_Prediction": 0.58275632076935, "ARES_Confidence_Interval": [0.514, 0.651], "Number_of_Examples_in_Evaluation_Set": 25788, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.656, "Annotated_Examples_used_for_PPI": 300}]]