[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.4703806672369546, "ARES_Confidence_Interval": [0.413, 0.528], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.924, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.4946466007786763, "ARES_Confidence_Interval": [0.437, 0.553], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.943, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5123626373626373, "ARES_Confidence_Interval": [0.454, 0.571], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.943, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.539999179924553, "ARES_Confidence_Interval": [0.482, 0.598], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.945, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.5703806672369546, "ARES_Confidence_Interval": [0.512, 0.629], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.945, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.5731451756732655, "ARES_Confidence_Interval": [0.514, 0.632], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.951, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.5944805194805195, "ARES_Confidence_Interval": [0.535, 0.654], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.956, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.6270215633423181, "ARES_Confidence_Interval": [0.568, 0.686], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.965, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.6499250749250749, "ARES_Confidence_Interval": [0.591, 0.709], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 112}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.4792557741659538, "ARES_Confidence_Interval": [0.433, 0.526], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.983, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.49468403713686737, "ARES_Confidence_Interval": [0.448, 0.541], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.995, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5258241758241758, "ARES_Confidence_Interval": [0.479, 0.573], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.987, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.5544120059045432, "ARES_Confidence_Interval": [0.507, 0.602], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.985, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.5738665526090676, "ARES_Confidence_Interval": [0.526, 0.622], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.99, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.5997413946852149, "ARES_Confidence_Interval": [0.552, 0.648], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.985, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.624025974025974, "ARES_Confidence_Interval": [0.576, 0.672], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.991, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.6462264150943396, "ARES_Confidence_Interval": [0.598, 0.694], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.985, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.6747752247752248, "ARES_Confidence_Interval": [0.627, 0.723], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.99, "Annotated_Examples_used_for_PPI": 112}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.499001996007984, "ARES_Confidence_Interval": [0.406, 0.592], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.499, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.49895178197064993, "ARES_Confidence_Interval": [0.406, 0.592], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.524, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5, "ARES_Confidence_Interval": [0.407, 0.593], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.551, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.5, "ARES_Confidence_Interval": [0.407, 0.593], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.575, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.5, "ARES_Confidence_Interval": [0.407, 0.593], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.6, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.5, "ARES_Confidence_Interval": [0.407, 0.593], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.625, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.5, "ARES_Confidence_Interval": [0.407, 0.593], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.651, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.5, "ARES_Confidence_Interval": [0.407, 0.593], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.675, "Annotated_Examples_used_for_PPI": 112}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.5, "ARES_Confidence_Interval": [0.407, 0.593], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.701, "Annotated_Examples_used_for_PPI": 112}]]